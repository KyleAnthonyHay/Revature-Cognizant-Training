{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eba42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 02: Embedding Exploration - Starter Code\n",
    "\n",
    "Complete the TODOs to explore text embeddings and similarity.\n",
    "\n",
    "Prerequisites:\n",
    "- pip install numpy sentence-transformers\n",
    "\n",
    "Hints:\n",
    "- Demo 02 (demo_02_embedding_generation.py) shows model loading\n",
    "- Reading 05 (vector-similarity-concepts.md) has the cosine formula\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "# TODO 1.1: Import SentenceTransformer from sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd241409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Part 1: Loading the Embedding Model\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3560f6806cf44bc9841dd5834092acd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f2f79746a7447cb41173fd433a8239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dade661fc49d48f7adbc1c774f853ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba8829f25b84d13961b1d6c60b1f5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc997a16cf4c4198b387a28220b38277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a594feddb9c45d1a2314e21928fd1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df75f3f019a4a82833d73617b88f526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00566adaccdb46859f3e1f17bf73a07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d9b81e30a64b65b91724307aea6a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e17257fa8a478aa09b0cddb0dd6566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f6b712e9f844c6841bd26b5f142e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 1: Load the Embedding Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Loading the Embedding Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO 1.1: Load the 'all-MiniLM-L6-v2' model\n",
    "# Hint: model = SentenceTransformer('...')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# TODO 1.2: Print the embedding dimension\n",
    "# Hint: Look for a method with \"embedding\" and \"dimension\" in the name\n",
    "dimensions = model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {dimensions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb5545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part 2: Generate Embeddings\n",
      "============================================================\n",
      "Shape: (384,)\n",
      "First 5 values: [ 0.00736547 -0.0427747   0.06799109 -0.00174222  0.06531589]\n",
      "Min: -0.12663623690605164, Max: 0.1434013843536377\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 2: Generate Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Generate Embeddings\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO 2.1: Encode a single sentence\n",
    "single_sentence = \"Machine learning is transforming industries\"\n",
    "# Hint: embedding = model.encode(...)\n",
    "single_embedding = model.encode(single_sentence) \n",
    "\n",
    "# Print embedding properties\n",
    "print(f\"Shape: {single_embedding.shape if single_embedding is not None else '???'}\")\n",
    "print(f\"First 5 values: {single_embedding[:5]}\")\n",
    "print(f\"Min: {np.min(single_embedding)}, Max: {np.max(single_embedding)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce2978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch shape: (6, 384)\n"
     ]
    }
   ],
   "source": [
    "# TODO 2.2: Batch encode these sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A kitten rested on the rug\",\n",
    "    \"Dogs are loyal companions\",\n",
    "    \"Python is a programming language\",\n",
    "    \"The python snake is quite long\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Hint: Pass the list directly to model.encode()\n",
    "embeddings = model.encode(sentences) \n",
    "\n",
    "print(f\"\\nBatch shape: {embeddings.shape if embeddings is not None else '???'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ccc500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part 3: Calculate Similarities\n",
      "============================================================\n",
      "\n",
      "Similarity Matrix:\n",
      "----------------------------------------\n",
      "1.0000 0.6131 0.1646 0.0309 0.1345 0.0337 \n",
      "0.6131 1.0000 0.1545 0.0472 0.1288 0.0294 \n",
      "0.1646 0.1545 1.0000 0.1078 0.0822 0.1410 \n",
      "0.0309 0.0472 0.1078 1.0000 0.4421 0.7304 \n",
      "0.1345 0.1288 0.0822 0.4421 1.0000 0.4036 \n",
      "0.0337 0.0294 0.1410 0.7304 0.4036 1.0000 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 3: Calculate Similarities\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Calculate Similarities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO 3.1: Implement cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Formula: cos(θ) = (A · B) / (||A|| × ||B||)\n",
    "    \n",
    "    Hint: Use np.dot() for dot product\n",
    "    Hint: Use np.linalg.norm() for magnitude\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    dot_product = np.dot(a, b)\n",
    "    magnitude_a = np.linalg.norm(a)\n",
    "    magnitude_b = np.linalg.norm(b)\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "# TODO 3.2: Build similarity matrix\n",
    "# For each pair of sentences, calculate similarity\n",
    "print(\"\\nSimilarity Matrix:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Example structure (fill in the actual calculations):\n",
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        similarity_matrix[i, j] = sim\n",
    "        print(f\"{sim:.4f}\", end=\" \")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3.3: Analysis Questions\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3.3: Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Answer these questions based on your similarity matrix\n",
    "\n",
    "print(\"\"\"\n",
    "Q1: Which two sentences have the highest similarity (besides identical)?\n",
    "    Answer: sentences 3 and 5 (similarity: 0.7304)\n",
    "\n",
    "Q2: How similar are 'Python is a programming language' and 'The python snake is quite long'?\n",
    "    Similarity score: score: 0.4421\n",
    "    Interpretation: Not very similar\n",
    "\n",
    "Q3: Which sentence is most 'isolated' (lowest average similarity)?\n",
    "    Answer: Sentence 2 (\"Dogs are loyal companions\") with average similarity 0.1300\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0c6bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part 4: Semantic Clustering\n",
      "============================================================\n",
      "\n",
      "Cluster A (Animal-related):\n",
      "    - \"The cat sat on the mat\" (sentence 0)\n",
      "    - \"A kitten rested on the rug\" (sentence 1)\n",
      "    - \"Dogs are loyal companions\" (sentence 2)\n",
      "\n",
      "Cluster B (Programming-related):\n",
      "    - \"Python is a programming language\" (sentence 3)\n",
      "    - \"I love coding in Python\" (sentence 5)\n",
      "\n",
      "Outliers:\n",
      "    - \"The python snake is quite long\" (sentence 4) - ambiguous between animal and programming contexts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 4: Semantic Clustering\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Semantic Clustering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO 4.1: Group sentences by topic\n",
    "print(\"\"\"\n",
    "Cluster A (Animal-related):\n",
    "    - \"The cat sat on the mat\" (sentence 0)\n",
    "    - \"A kitten rested on the rug\" (sentence 1)\n",
    "    - \"Dogs are loyal companions\" (sentence 2)\n",
    "\n",
    "Cluster B (Programming-related):\n",
    "    - \"Python is a programming language\" (sentence 3)\n",
    "    - \"I love coding in Python\" (sentence 5)\n",
    "\n",
    "Outliers:\n",
    "    - \"The python snake is quite long\" (sentence 4) - ambiguous between animal and programming contexts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2: What similarity threshold would you use?\n",
    "print(\"\"\"\n",
    "Recommended threshold for 'related' sentences: ???\n",
    "Justification:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Exercise Complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
