{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3770376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 04: Production Pipeline - Starter Code\n",
    "\n",
    "Build a complete production-ready training pipeline.\n",
    "\n",
    "This is a capstone exercise combining all Friday topics.\n",
    "\n",
    "Prerequisites:\n",
    "- All Friday readings and demos\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78872cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRODUCTION TRAINER CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ProductionTrainer:\n",
    "    \"\"\"\n",
    "    Production-ready training pipeline.\n",
    "    \n",
    "    FEATURES TO IMPLEMENT:\n",
    "    1. Automatic checkpointing (save best model)\n",
    "    2. Early stopping (prevent overfitting)\n",
    "    3. TensorBoard logging (visualization)\n",
    "    4. Model versioning (timestamped runs)\n",
    "    5. Configuration saving (reproducibility)\n",
    "    6. Regularization (dropout + L2)\n",
    "    \n",
    "    DIRECTORY STRUCTURE:\n",
    "    production_runs/\n",
    "      modelname_20240115_143000/\n",
    "        checkpoints/\n",
    "          best_model.keras\n",
    "        logs/\n",
    "          train/\n",
    "          validation/\n",
    "        models/\n",
    "          final_model.keras\n",
    "        config.json\n",
    "        summary.json\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, output_dir='production_runs'):\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Versioned run directory\n",
    "        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.run_dir = os.path.join(output_dir, f'{model_name}_{self.timestamp}')\n",
    "        \n",
    "        # Subdirectories\n",
    "        self.checkpoint_dir = os.path.join(self.run_dir, 'checkpoints')\n",
    "        self.log_dir = os.path.join(self.run_dir, 'logs')\n",
    "        self.model_dir = os.path.join(self.run_dir, 'models')\n",
    "        \n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config = {}\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"\n",
    "        Create all output directories.\n",
    "        Use os.makedirs(path, exist_ok=True)\n",
    "        \"\"\"\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "    \n",
    "    def build_model(self, input_shape, num_classes, \n",
    "                    hidden_layers=[128, 64], \n",
    "                    dropout_rate=0.3, \n",
    "                    l2_lambda=0.001):\n",
    "        \"\"\"\n",
    "        Build model with regularization.\n",
    "        \n",
    "        ARCHITECTURE:\n",
    "        Input -> [Dense + Dropout] x N -> Output\n",
    "        \n",
    "        STORE CONFIG for reproducibility:\n",
    "        self.config = {\n",
    "            'input_shape': input_shape,\n",
    "            'num_classes': num_classes,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'l2_lambda': l2_lambda\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.config = {\n",
    "            'input_shape': input_shape,\n",
    "            'num_classes': num_classes,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'l2_lambda': l2_lambda\n",
    "        }\n",
    "        \n",
    "        self.model = keras.Sequential()\n",
    "        self.model.add(layers.Input(shape=input_shape))\n",
    "        \n",
    "        for units in hidden_layers:\n",
    "            self.model.add(layers.Dense(\n",
    "                units, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(l2_lambda)\n",
    "            ))\n",
    "            self.model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _create_callbacks(self, patience=10):\n",
    "        \"\"\"\n",
    "        Create production callbacks.\n",
    "        \n",
    "        CALLBACKS TO CREATE:\n",
    "        1. ModelCheckpoint - save best model to checkpoint_dir\n",
    "        2. EarlyStopping - patience epochs, restore_best_weights=True\n",
    "        3. TensorBoard - log to log_dir\n",
    "        4. CSVLogger - save metrics to CSV\n",
    "        5. ReduceLROnPlateau - reduce LR when stuck\n",
    "        \n",
    "        SEE: demo_02_checkpoint_callback.py for ModelCheckpoint\n",
    "        SEE: demo_03_early_stopping.py for EarlyStopping\n",
    "        \"\"\"\n",
    "        callbacks = []\n",
    "        \n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(self.checkpoint_dir, 'best_model.keras'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(checkpoint)\n",
    "        \n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "        tensorboard = keras.callbacks.TensorBoard(\n",
    "            log_dir=self.log_dir,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "        callbacks.append(tensorboard)\n",
    "        \n",
    "        csv_logger = keras.callbacks.CSVLogger(\n",
    "            os.path.join(self.run_dir, 'training_log.csv')\n",
    "        )\n",
    "        callbacks.append(csv_logger)\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=patience // 2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(reduce_lr)\n",
    "        \n",
    "        return callbacks\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val,\n",
    "              epochs=100, batch_size=32, patience=10):\n",
    "        \"\"\"\n",
    "        Run training with all callbacks.\n",
    "        \n",
    "        STEPS:\n",
    "        1. Setup directories\n",
    "        2. Create callbacks\n",
    "        3. Train model\n",
    "        4. Save final model\n",
    "        5. Save config and summary\n",
    "        \"\"\"\n",
    "        self._setup_directories()\n",
    "        \n",
    "        callbacks = self._create_callbacks(patience=patience)\n",
    "        \n",
    "        self.config.update({\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'patience': patience\n",
    "        })\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.model.save(os.path.join(self.model_dir, 'final_model.keras'))\n",
    "        \n",
    "        self._save_config()\n",
    "        self._save_summary()\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def _save_config(self):\n",
    "        \"\"\"\n",
    "        Save config.json with all hyperparameters.\n",
    "        Use json.dump(self.config, f, indent=2)\n",
    "        \"\"\"\n",
    "        config_path = os.path.join(self.run_dir, 'config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "    \n",
    "    def _save_summary(self):\n",
    "        \"\"\"\n",
    "        Save summary.json with training results.\n",
    "        \n",
    "        INCLUDE:\n",
    "        - Best val_accuracy\n",
    "        - Best val_loss\n",
    "        - Total epochs trained\n",
    "        - Early stopped epoch (if applicable)\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'best_val_accuracy': float(max(self.history.history['val_accuracy'])),\n",
    "            'best_val_loss': float(min(self.history.history['val_loss'])),\n",
    "            'total_epochs_trained': len(self.history.history['loss']),\n",
    "            'configured_epochs': self.config.get('epochs', None),\n",
    "            'early_stopped': len(self.history.history['loss']) < self.config.get('epochs', 0)\n",
    "        }\n",
    "        \n",
    "        summary_path = os.path.join(self.run_dir, 'summary.json')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "    \n",
    "    def load_best_model(self):\n",
    "        \"\"\"\n",
    "        Load the best checkpoint.\n",
    "        Return: loaded model\n",
    "        \"\"\"\n",
    "        best_model_path = os.path.join(self.checkpoint_dir, 'best_model.keras')\n",
    "        return keras.models.load_model(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST THE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def test_production_pipeline():\n",
    "    \"\"\"\n",
    "    Test the complete pipeline with MNIST.\n",
    "    \n",
    "    STEPS:\n",
    "    1. Load MNIST data\n",
    "    2. Create ProductionTrainer\n",
    "    3. Build model\n",
    "    4. Train\n",
    "    5. Load best model and evaluate on test set\n",
    "    6. Print summary of created files\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing Production Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "    \n",
    "    # Use subset for faster testing\n",
    "    x_train, y_train = x_train[:5000], y_train[:5000]\n",
    "    \n",
    "    # Validation split\n",
    "    x_val, y_val = x_train[-1000:], y_train[-1000:]\n",
    "    x_train, y_train = x_train[:-1000], y_train[:-1000]\n",
    "    \n",
    "    # YOUR CODE:\n",
    "    # 1. Create trainer\n",
    "    trainer = ProductionTrainer('mnist_classifier')\n",
    "    \n",
    "    # 2. Build model\n",
    "    trainer.build_model(input_shape=(784,), num_classes=10)\n",
    "    \n",
    "    # 3. Train\n",
    "    trainer.train(x_train, y_train, x_val, y_val, epochs=50)\n",
    "    \n",
    "    # 4. Load best and evaluate\n",
    "    best_model = trainer.load_best_model()\n",
    "    test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # 5. List created files\n",
    "    print(\"\\\\nCreated files:\")\n",
    "    for root, dirs, files in os.walk(trainer.run_dir):\n",
    "        for f in files:\n",
    "            print(f\"  {os.path.join(root, f)}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49fd0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Exercise 04: Production Pipeline\n",
      "============================================================\n",
      "============================================================\n",
      "Testing Production Pipeline\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m102/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 998us/step - accuracy: 0.3937 - loss: 2.0666\n",
      "Epoch 1: val_loss improved from None to 0.73297, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 1: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6080 - loss: 1.4923 - val_accuracy: 0.8540 - val_loss: 0.7330 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m 64/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.8012 - loss: 0.8618\n",
      "Epoch 2: val_loss improved from 0.73297 to 0.55409, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 2: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8298 - loss: 0.7844 - val_accuracy: 0.8970 - val_loss: 0.5541 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 826us/step - accuracy: 0.8645 - loss: 0.6335\n",
      "Epoch 3: val_loss improved from 0.55409 to 0.51753, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 3: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8775 - loss: 0.6183 - val_accuracy: 0.9050 - val_loss: 0.5175 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m 70/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 728us/step - accuracy: 0.9041 - loss: 0.5142\n",
      "Epoch 4: val_loss improved from 0.51753 to 0.46497, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 4: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9020 - loss: 0.5282 - val_accuracy: 0.9150 - val_loss: 0.4650 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m 95/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9212 - loss: 0.4603\n",
      "Epoch 5: val_loss improved from 0.46497 to 0.45759, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 5: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9187 - loss: 0.4796 - val_accuracy: 0.9270 - val_loss: 0.4576 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 756us/step - accuracy: 0.9157 - loss: 0.4578\n",
      "Epoch 6: val_loss improved from 0.45759 to 0.42692, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 6: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9220 - loss: 0.4394 - val_accuracy: 0.9260 - val_loss: 0.4269 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 739us/step - accuracy: 0.9334 - loss: 0.4132\n",
      "Epoch 7: val_loss did not improve from 0.42692\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9390 - loss: 0.4044 - val_accuracy: 0.9270 - val_loss: 0.4309 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 741us/step - accuracy: 0.9448 - loss: 0.3665\n",
      "Epoch 8: val_loss improved from 0.42692 to 0.41926, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 8: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9400 - loss: 0.3935 - val_accuracy: 0.9240 - val_loss: 0.4193 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.9529 - loss: 0.3541\n",
      "Epoch 9: val_loss improved from 0.41926 to 0.41289, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 9: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9480 - loss: 0.3622 - val_accuracy: 0.9270 - val_loss: 0.4129 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - accuracy: 0.9478 - loss: 0.3474\n",
      "Epoch 10: val_loss improved from 0.41289 to 0.39611, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 10: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9500 - loss: 0.3459 - val_accuracy: 0.9400 - val_loss: 0.3961 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m 67/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 762us/step - accuracy: 0.9608 - loss: 0.3173\n",
      "Epoch 11: val_loss improved from 0.39611 to 0.38767, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 11: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9545 - loss: 0.3338 - val_accuracy: 0.9400 - val_loss: 0.3877 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m 65/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 792us/step - accuracy: 0.9591 - loss: 0.3139\n",
      "Epoch 12: val_loss did not improve from 0.38767\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9605 - loss: 0.3122 - val_accuracy: 0.9390 - val_loss: 0.3886 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m 64/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 800us/step - accuracy: 0.9685 - loss: 0.2871\n",
      "Epoch 13: val_loss improved from 0.38767 to 0.38626, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 13: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9638 - loss: 0.2962 - val_accuracy: 0.9390 - val_loss: 0.3863 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 775us/step - accuracy: 0.9667 - loss: 0.2838\n",
      "Epoch 14: val_loss did not improve from 0.38626\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9605 - loss: 0.3019 - val_accuracy: 0.9410 - val_loss: 0.3908 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 771us/step - accuracy: 0.9694 - loss: 0.2647\n",
      "Epoch 15: val_loss did not improve from 0.38626\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.2832 - val_accuracy: 0.9370 - val_loss: 0.3910 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m 67/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 758us/step - accuracy: 0.9637 - loss: 0.2791\n",
      "Epoch 16: val_loss improved from 0.38626 to 0.38440, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 16: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9653 - loss: 0.2779 - val_accuracy: 0.9360 - val_loss: 0.3844 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9748 - loss: 0.2635\n",
      "Epoch 17: val_loss improved from 0.38440 to 0.38104, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 17: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9680 - loss: 0.2757 - val_accuracy: 0.9390 - val_loss: 0.3810 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.9534 - loss: 0.2875\n",
      "Epoch 18: val_loss improved from 0.38104 to 0.35157, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 18: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9613 - loss: 0.2750 - val_accuracy: 0.9470 - val_loss: 0.3516 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 691us/step - accuracy: 0.9749 - loss: 0.2497\n",
      "Epoch 19: val_loss did not improve from 0.35157\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.9675 - loss: 0.2587 - val_accuracy: 0.9450 - val_loss: 0.3635 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m 70/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 733us/step - accuracy: 0.9675 - loss: 0.2650\n",
      "Epoch 20: val_loss did not improve from 0.35157\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9718 - loss: 0.2534 - val_accuracy: 0.9450 - val_loss: 0.3682 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 778us/step - accuracy: 0.9739 - loss: 0.2347\n",
      "Epoch 21: val_loss did not improve from 0.35157\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9675 - loss: 0.2559 - val_accuracy: 0.9440 - val_loss: 0.3724 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m 64/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 799us/step - accuracy: 0.9810 - loss: 0.2225\n",
      "Epoch 22: val_loss did not improve from 0.35157\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9725 - loss: 0.2396 - val_accuracy: 0.9450 - val_loss: 0.3549 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.9807 - loss: 0.2216\n",
      "Epoch 23: val_loss did not improve from 0.35157\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9735 - loss: 0.2415 - val_accuracy: 0.9370 - val_loss: 0.3631 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m 67/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 768us/step - accuracy: 0.9732 - loss: 0.2389\n",
      "Epoch 24: val_loss improved from 0.35157 to 0.33650, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 24: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9770 - loss: 0.2268 - val_accuracy: 0.9500 - val_loss: 0.3365 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m 67/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 764us/step - accuracy: 0.9819 - loss: 0.2119\n",
      "Epoch 25: val_loss did not improve from 0.33650\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9835 - loss: 0.2068 - val_accuracy: 0.9470 - val_loss: 0.3378 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 710us/step - accuracy: 0.9821 - loss: 0.2060\n",
      "Epoch 26: val_loss did not improve from 0.33650\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - accuracy: 0.9825 - loss: 0.2024 - val_accuracy: 0.9500 - val_loss: 0.3373 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m 75/125\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.9847 - loss: 0.1918\n",
      "Epoch 27: val_loss did not improve from 0.33650\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - accuracy: 0.9845 - loss: 0.1955 - val_accuracy: 0.9480 - val_loss: 0.3375 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.9845 - loss: 0.1937\n",
      "Epoch 28: val_loss improved from 0.33650 to 0.33617, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 28: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9830 - loss: 0.1972 - val_accuracy: 0.9430 - val_loss: 0.3362 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m101/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9890 - loss: 0.1876\n",
      "Epoch 29: val_loss improved from 0.33617 to 0.32397, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 29: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9858 - loss: 0.1916 - val_accuracy: 0.9480 - val_loss: 0.3240 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m 65/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 788us/step - accuracy: 0.9796 - loss: 0.1916\n",
      "Epoch 30: val_loss did not improve from 0.32397\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9843 - loss: 0.1865 - val_accuracy: 0.9390 - val_loss: 0.3409 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9875 - loss: 0.1819\n",
      "Epoch 31: val_loss did not improve from 0.32397\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9852 - loss: 0.1861 - val_accuracy: 0.9490 - val_loss: 0.3312 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.9886 - loss: 0.1818\n",
      "Epoch 32: val_loss did not improve from 0.32397\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 0.9862 - loss: 0.1817 - val_accuracy: 0.9400 - val_loss: 0.3374 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.9840 - loss: 0.1742\n",
      "Epoch 33: val_loss did not improve from 0.32397\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.9825 - loss: 0.1811 - val_accuracy: 0.9430 - val_loss: 0.3256 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 720us/step - accuracy: 0.9891 - loss: 0.1730\n",
      "Epoch 34: val_loss improved from 0.32397 to 0.32257, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 34: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9847 - loss: 0.1812 - val_accuracy: 0.9420 - val_loss: 0.3226 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 0.9886 - loss: 0.1720\n",
      "Epoch 35: val_loss improved from 0.32257 to 0.32035, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 35: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9855 - loss: 0.1804 - val_accuracy: 0.9460 - val_loss: 0.3203 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 0.9856 - loss: 0.1786\n",
      "Epoch 36: val_loss improved from 0.32035 to 0.31213, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 36: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9833 - loss: 0.1850 - val_accuracy: 0.9400 - val_loss: 0.3121 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m 64/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.9869 - loss: 0.1721\n",
      "Epoch 37: val_loss did not improve from 0.31213\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9860 - loss: 0.1747 - val_accuracy: 0.9420 - val_loss: 0.3193 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m 65/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 786us/step - accuracy: 0.9876 - loss: 0.1628\n",
      "Epoch 38: val_loss did not improve from 0.31213\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9847 - loss: 0.1716 - val_accuracy: 0.9430 - val_loss: 0.3149 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 0.9902 - loss: 0.1710\n",
      "Epoch 39: val_loss improved from 0.31213 to 0.30135, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 39: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9835 - loss: 0.1765 - val_accuracy: 0.9470 - val_loss: 0.3013 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9865 - loss: 0.1670\n",
      "Epoch 40: val_loss did not improve from 0.30135\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.9855 - loss: 0.1682 - val_accuracy: 0.9480 - val_loss: 0.3189 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9849 - loss: 0.1734\n",
      "Epoch 41: val_loss did not improve from 0.30135\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 0.9845 - loss: 0.1727 - val_accuracy: 0.9470 - val_loss: 0.3120 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9873 - loss: 0.1621\n",
      "Epoch 42: val_loss did not improve from 0.30135\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - accuracy: 0.9862 - loss: 0.1669 - val_accuracy: 0.9460 - val_loss: 0.3062 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 0.9872 - loss: 0.1696\n",
      "Epoch 43: val_loss did not improve from 0.30135\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9847 - loss: 0.1693 - val_accuracy: 0.9500 - val_loss: 0.3083 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 746us/step - accuracy: 0.9864 - loss: 0.1661\n",
      "Epoch 44: val_loss did not improve from 0.30135\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.1610 - val_accuracy: 0.9450 - val_loss: 0.3093 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 0.9842 - loss: 0.1598\n",
      "Epoch 45: val_loss improved from 0.30135 to 0.30112, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 45: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9883 - loss: 0.1569 - val_accuracy: 0.9500 - val_loss: 0.3011 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m 70/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.9925 - loss: 0.1406\n",
      "Epoch 46: val_loss improved from 0.30112 to 0.29672, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 46: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9890 - loss: 0.1527 - val_accuracy: 0.9480 - val_loss: 0.2967 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 723us/step - accuracy: 0.9896 - loss: 0.1465\n",
      "Epoch 47: val_loss did not improve from 0.29672\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - accuracy: 0.9898 - loss: 0.1485 - val_accuracy: 0.9480 - val_loss: 0.3075 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.9906 - loss: 0.1487\n",
      "Epoch 48: val_loss improved from 0.29672 to 0.29666, saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 48: finished saving model to production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9910 - loss: 0.1481 - val_accuracy: 0.9500 - val_loss: 0.2967 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.9927 - loss: 0.1432\n",
      "Epoch 49: val_loss did not improve from 0.29666\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.9935 - loss: 0.1411 - val_accuracy: 0.9440 - val_loss: 0.3018 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 0.9871 - loss: 0.1533\n",
      "Epoch 50: val_loss did not improve from 0.29666\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9900 - loss: 0.1453 - val_accuracy: 0.9470 - val_loss: 0.2993 - learning_rate: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - accuracy: 0.9478 - loss: 0.2830\n",
      "Test accuracy: 0.9478\n",
      "\n",
      "Created files:\n",
      "  production_runs/mnist_classifier_20251227_213744/summary.json\n",
      "  production_runs/mnist_classifier_20251227_213744/config.json\n",
      "  production_runs/mnist_classifier_20251227_213744/training_log.csv\n",
      "  production_runs/mnist_classifier_20251227_213744/checkpoints/best_model.keras\n",
      "  production_runs/mnist_classifier_20251227_213744/models/final_model.keras\n",
      "  production_runs/mnist_classifier_20251227_213744/logs/train/events.out.tfevents.1766889464.Mac.22749.0.v2\n",
      "  production_runs/mnist_classifier_20251227_213744/logs/validation/events.out.tfevents.1766889465.Mac.22749.1.v2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Exercise 04: Production Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Uncomment to test:\n",
    "    test_production_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
